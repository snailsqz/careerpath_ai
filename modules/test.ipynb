{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f24456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch process...\n",
      "Page 1: Kept 79 courses.\n",
      "Page 2: Kept 76 courses.\n",
      "Page 3: Kept 70 courses.\n",
      "Page 4: Kept 78 courses.\n",
      "Page 5: Kept 86 courses.\n",
      "Saved 389 courses to coursera_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch_courses(limit_per_page=100, max_pages=5):\n",
    "    base_url = \"https://api.coursera.org/api/courses.v1\"\n",
    "    \n",
    "    fields = \"name,description,slug,level,primaryLanguages,workload,domainTypes,certificates\"\n",
    "    \n",
    "    all_courses = []\n",
    "    start = 0\n",
    "    page_count = 0\n",
    "    \n",
    "    print(f\"Starting fetch process...\")\n",
    "    \n",
    "    while page_count < max_pages:\n",
    "        params = {\n",
    "            \"start\": start,\n",
    "            \"limit\": limit_per_page,\n",
    "            \"fields\": fields\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                elements = data.get('elements', [])\n",
    "                \n",
    "                if not elements:\n",
    "                    print(\"No more data available.\")\n",
    "                    break\n",
    "                \n",
    "                filtered_count = 0\n",
    "                for item in elements:\n",
    "                    languages = item.get(\"primaryLanguages\", [])\n",
    "                    if 'en' not in languages:\n",
    "                        continue\n",
    "\n",
    "                    domains = item.get(\"domainTypes\", [])\n",
    "                    if domains:\n",
    "                        category = domains[0].get(\"subdomainId\") or domains[0].get(\"domainId\") or \"General\"\n",
    "                    else:\n",
    "                        category = \"General\"\n",
    "\n",
    "                    certs = item.get(\"certificates\", [])\n",
    "                    cert_str = \", \".join(certs) if certs else \"Standard Course Certificate\"\n",
    "\n",
    "                    course_info = {\n",
    "                        \"id\": item.get(\"id\"),\n",
    "                        \"title\": item.get(\"name\"),\n",
    "                        \"description\": item.get(\"description\"),\n",
    "                        \"level\": item.get(\"level\", \"Not Specified\"),\n",
    "                        \"duration\": item.get(\"workload\", \"Self-paced\"),\n",
    "                        \"category\": category,\n",
    "                        \"certificate_type\": cert_str,\n",
    "                        \"url\": f\"https://www.coursera.org/learn/{item.get('slug')}\"\n",
    "                    }\n",
    "                    all_courses.append(course_info)\n",
    "                    filtered_count += 1\n",
    "                \n",
    "                print(f\"Page {page_count + 1}: Kept {filtered_count} courses.\")\n",
    "                \n",
    "                if 'paging' in data and 'next' in data['paging']:\n",
    "                    start = int(data['paging']['next'])\n",
    "                    page_count += 1\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Error: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_courses\n",
    "\n",
    "def save_to_csv(courses, filename=\"coursera_dataset.csv\"):\n",
    "    if not courses:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(courses)\n",
    "    df = df.dropna(subset=['description']) \n",
    "    \n",
    "    if 'category' in df.columns:\n",
    "        df['category'] = df['category'].str.replace('-', ' ').str.title()\n",
    "\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved {len(df)} courses to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    courses = fetch_courses(max_pages=5)\n",
    "    save_to_csv(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e904409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. อ่านไฟล์ CSV\n",
    "df = pd.read_csv(\"coursera_dataset.csv\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)  # ให้โชว์แค่ 50 ตัวอักษรพอ เดี๋ยวล้นจอ\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-chroma langchain-huggingface chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a774ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\snails\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\snails\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\snails\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 389 courses...\n",
      "Creating Vector Database...\n",
      "Database saved to ./vector_store\n",
      "\n",
      "Testing Search: 'I want to learn about Python for Data Science'\n",
      "1. Python Programming Fundamentals (BEGINNER)\n",
      "   Link: https://www.coursera.org/learn/microsoft-python-programming-fundamentals\n",
      "--------------------\n",
      "2. Intermediate Python – Libraries, Tools & Practical Projects (INTERMEDIATE)\n",
      "   Link: https://www.coursera.org/learn/packt-intermediate-python-libraries-tools-and-practical-projects-0d9as\n",
      "--------------------\n",
      "3. Data Visualization and Modeling in Python (INTERMEDIATE)\n",
      "   Link: https://www.coursera.org/learn/python-data-modeling\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def build_database(csv_path=\"coursera_dataset.csv\", db_path=\"./vector_store\"):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # ใช้ Model ฟรีของ HuggingFace ทำงานบน CPU ได้ ไม่ต้องใช้ API Key\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    documents = []\n",
    "    print(f\"Processing {len(df)} courses...\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # รวมข้อมูลที่จะให้ AI ใช้ค้นหาความหมาย\n",
    "        content = f\"\"\"\n",
    "        Title: {row['title']}\n",
    "        Category: {row['category']}\n",
    "        Level: {row['level']}\n",
    "        Description: {row['description']}\n",
    "        \"\"\"\n",
    "        \n",
    "        # เก็บข้อมูลสำหรับนำไปแสดงผล (AI ไม่เอาไปคำนวณ แต่เก็บไว้ให้)\n",
    "        metadata = {\n",
    "            \"id\": str(row['id']),\n",
    "            \"title\": row['title'],\n",
    "            \"url\": row['url'],\n",
    "            \"duration\": str(row['duration']),\n",
    "            \"certificate\": str(row['certificate_type']),\n",
    "            \"level\": str(row['level'])\n",
    "        }\n",
    "\n",
    "        doc = Document(page_content=content.strip(), metadata=metadata)\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(\"Creating Vector Database...\")\n",
    "    \n",
    "    # สร้าง DB และบันทึกลงโฟลเดอร์ (Persist)\n",
    "    Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    print(f\"Database saved to {db_path}\")\n",
    "\n",
    "def test_search(query, db_path=\"./vector_store\"):\n",
    "    print(f\"\\nTesting Search: '{query}'\")\n",
    "    \n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    db = Chroma(persist_directory=db_path, embedding_function=embedding_model)\n",
    "    \n",
    "    # ค้นหา 3 อันดับแรกที่ใกล้เคียงที่สุด\n",
    "    results = db.similarity_search(query, k=3)\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"{i+1}. {doc.metadata['title']} ({doc.metadata['level']})\")\n",
    "        print(f\"   Link: {doc.metadata['url']}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. สร้าง DB\n",
    "    build_database()\n",
    "    \n",
    "    # 2. ลองค้นหาดู\n",
    "    test_search(\"I want to learn about Python for Data Science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7585a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Search: 'i want to learn TailwindCSS'\n",
      "1. Tailwind CSS Practice Project: Build a Product Card (INTERMEDIATE)\n",
      "   Link: https://www.coursera.org/learn/build-a-product-card-with-tailwind-css\n",
      "--------------------\n",
      "2. Create a Dark Moody Atmospheric 2D Game with Unity and C# (BEGINNER)\n",
      "   Link: https://www.coursera.org/learn/packt-create-a-dark-moody-atmospheric-2d-game-with-unity-and-c-2qqm9\n",
      "--------------------\n",
      "3. Unix System Overview and Command (INTERMEDIATE)\n",
      "   Link: https://www.coursera.org/learn/unix-system-overview-and-command\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "test_search(\"i want to learn TailwindCSS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e35089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: c:\\SUPERPROJECT\\modules\n",
      "Looking for CSV at: c:\\SUPERPROJECT\\modules\\data\\coursera_dataset.csv\n",
      "Error: File not found at c:\\SUPERPROJECT\\modules\\data\\coursera_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    current_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    current_path = os.getcwd() # กันเหนียวเผื่อรันใน Notebook\n",
    "\n",
    "project_root = current_path\n",
    "    \n",
    "    # 2. วนลูปเดินถอยหลังขึ้นไปเรื่อยๆ จนกว่าจะเจอโฟลเดอร์ชื่อ 'data'\n",
    "while True:\n",
    "    possible_data_path = os.path.join(project_root, 'data')\n",
    "    \n",
    "    if os.path.exists(possible_data_path):\n",
    "        # เจอแล้ว! หยุดตรงนี้แหละคือ Root\n",
    "        print(f\"✅ Found project root at: {project_root}\")\n",
    "        break\n",
    "    \n",
    "    # ถ้าไม่เจอ ให้ถอยหลังขึ้นไปอีก 1 ชั้น\n",
    "    parent_dir = os.path.dirname(project_root)\n",
    "    \n",
    "    if parent_dir == project_root:\n",
    "        # ถ้าถอยจนสุดทาง (C:\\) แล้วยังไม่เจอ แสดงว่าวางไฟล์ผิดที่\n",
    "        print(\"❌ Error: Could not find 'data' folder.\")\n",
    "        print(\"Please check if 'data' folder exists inside your project.\")\n",
    "        \n",
    "    project_root = parent_dir\n",
    "\n",
    "    # 3. กำหนด path ที่ถูกต้องจาก Root ที่หาเจอ\n",
    "csv_path = os.path.join(project_root, 'data', 'coursera_dataset.csv')\n",
    "db_path = os.path.join(project_root, 'vector_store')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
